{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Home Work 5</h1>\n",
    "\n",
    "<h2>Panther ID: 002615185</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 1)</h2>\n",
    "\n",
    "Reconstruction error is a learning parameter we use to train and optimize our NN or machine learning model. Any bias and business requirements can be added to the error so that the model will be trained accordingly. \n",
    "\n",
    "\n",
    "\n",
    "Regularization is a way to keep the model simple and prevent it from overfitting. \n",
    "\n",
    "Let's have a look at the below equation. \n",
    "\n",
    "h(x) = w3(x^3) + w2(x^2) + w1(x) + w0\n",
    "\n",
    "This equation shows that x with a high exponent will overfit and make the model more complex but gives a good value for the training and testing as the resultant curve passes through all the given points. \n",
    "\n",
    "Regularization will help you keep the model simple and make the weights of higher exponents zero or very low, keeping the model simple and preventing overfitting. \n",
    "\n",
    "Assume h(x) is out model function, and l_reg(w) = l(w) + l0(w)\n",
    "\n",
    "\n",
    "<strong><h2>L0:</h2></strong>  L0(w) results in the total no of non-zero weights. Using a non-zero number of parameters can be a helpful selection feature, with sparse features achieved when selection parameters are non-zero. \n",
    "\n",
    "L0: this can help in <b>parameter selection</b>, such as ignoring the parameters that have no or minimal impact on the output and only selecting the decision-making features.\n",
    "\n",
    "(Personal opinion) Here, the l0 can prefer a few weights with very high values over more weights with fewer values. \n",
    "\n",
    "\n",
    "<strong><h2>L1:</h2></strong> Is an absolute sum of all weights in the model. The size and value of the parameters are proportional to the model's complexity. So complex models have a big L1 norm, resulting in a huge loss function, indicating that this model is inadequate.\n",
    "\n",
    "L1 may not prevent overfitting, but the <b>important features will be given more weight</b>, and the normalization of parameters as per their scale will be achieved. <b>Feature selection</b> is done with it's help\n",
    "\n",
    "l1(w) = abs(w2 + w1 + w0)\n",
    "\n",
    "<b><h2>L2:</h2></b>  Is the sum of squares of all the weights in the model. L2 will reward the model when the weights are close to zero and below 1. As their squares will be almost zero. L2 will <b>prevent the model from overfitting. </b>\n",
    "\n",
    "l2(w) = (w2)^2 + (w1)^2 + (w0)^2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
