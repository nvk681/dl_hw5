{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Home Work 5</h1>\n",
    "\n",
    "<h2>Panther ID: 002615185</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 1)</h2>\n",
    "\n",
    "Reconstruction error is a learning parameter we use to train and optimize our NN or machine learning model. Any bias and business requirements can be added to the error so that the model will be trained accordingly. \n",
    "\n",
    "\n",
    "\n",
    "Regularization is a way to keep the model simple and prevent it from overfitting. \n",
    "\n",
    "Let's have a look at the below equation. \n",
    "\n",
    "h(x) = w3(x^3) + w2(x^2) + w1(x) + w0\n",
    "\n",
    "This equation shows that x with a high exponent will overfit and make the model more complex but gives a good value for the training and testing as the resultant curve passes through all the given points. \n",
    "\n",
    "Regularization will help you keep the model simple and make the weights of higher exponents zero or very low, keeping the model simple and preventing overfitting. \n",
    "\n",
    "Assume h(x) is out model function, and l_reg(w) = l(w) + l0(w)\n",
    "\n",
    "\n",
    "<strong><h2>L0:</h2></strong>  L0(w) results in the total no of non-zero weights. Using a non-zero number of parameters can be a helpful selection feature, with sparse features achieved when selection parameters are non-zero. \n",
    "\n",
    "L0: this can help in <b>parameter selection</b>, such as ignoring the parameters that have no or minimal impact on the output and only selecting the decision-making features.\n",
    "\n",
    "(Personal opinion) Here, the l0 can prefer a few weights with very high values over more weights with fewer values. \n",
    "\n",
    "\n",
    "<strong><h2>L1:</h2></strong> Is an absolute sum of all weights in the model. The size and value of the parameters are proportional to the model's complexity. So complex models have a big L1 norm, resulting in a huge loss function, indicating that this model is inadequate.\n",
    "\n",
    "L1 may not prevent overfitting, but the <b>important features will be given more weight</b>, and the normalization of parameters as per their scale will be achieved. <b>Feature selection</b> is done with it's help\n",
    "\n",
    "l1(w) = abs(w2 + w1 + w0)\n",
    "\n",
    "<b><h2>L2:</h2></b>  Is the sum of squares of all the weights in the model. L2 will reward the model when the weights are close to zero and below 1. As their squares will be almost zero. L2 will <b>prevent the model from overfitting. </b>\n",
    "\n",
    "l2(w) = (w2)^2 + (w1)^2 + (w0)^2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 2)</h1>\n",
    "\n",
    "<b>Contrast images with sharp geometric edge:</b>\n",
    "This property will create an easy solution. A high-resolution color image with a natural edge will have to consider many factors to create a loss function.\n",
    "\n",
    "My approach will be computing a basic PSNR to the generated and the sample image. As we need to minimize the loss, we use <b>-ve of PSNR.</b>\n",
    "\n",
    "<b>PSNR = 10Log10( (R^2)/MSE )</b>\n",
    "\n",
    "Here in place of R, we can use the range of the pixel map. The MSE is the mean square error for the pixel difference in the images. \n",
    "\n",
    "Justification: \n",
    "\n",
    "<b>Usage of MSE:</b> as we have a limited range of color pixels, the edges are shared, and any difference in the pixels can cause a significant change in the resulting image; we prevent such small or huge differences with the help of MSE. \n",
    "\n",
    "\n",
    "<b>Negative (PSNR):</b> PSNR is an excellent factor in measuring the compressed and reconstructed error quality. A negative value for it will be a great loss function to optimize the PSNR. \n",
    "\n",
    "\n",
    "<b>NOTE:</b> The upper and lower limits for the loss function are not considered. This may need to be tuned based on the Exact data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 3)</h1>\n",
    "\n",
    "A basic approach can be applying PSNR for each color scale. As the data set is animals in wildlife, this method can fail for many reasons. \n",
    "\n",
    "\n",
    "The approach will be using the loss function proposed or used in the paper \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.\"\n",
    "\n",
    "<b> Content loss: </b> We can utilize a loss function closer to perceptual similarity than pixel-wise losses. The updated Euclidean distance between the feature and the GAN loss is calculated. \n",
    "\n",
    "<img src=\"loss.png\">\n",
    "\n",
    "\n",
    "W, H being each node's respective feature maps of each respective node. \n",
    "\n",
    "\n",
    "<b> Adversarial loss: </b>\n",
    "\n",
    "We add the generative component of our GAN to the perceptual loss in addition to the content loss. Attempting to trick the discriminator network encourages our network to favor solutions based on various natural images. \n",
    "\n",
    "<b>L = Sigma( - LOG( D (G (I) ) ) )</b>\n",
    "\n",
    "<b>D (G (I) ):</b> The probability that the reconstructed image, G(I)\n",
    "being an actual image.\n",
    "\n",
    "\n",
    "This approach gives a bit of robustness, and all will give enough relaxation to generate HD images through GAN, where sharp edges can exist in fewer numbers and address a color image. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
