{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Home Work 5</h1>\n",
    "\n",
    "<h2>Panther ID: 002615185</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question 1)</h2>\n",
    "\n",
    "Reconstruction error is a learning parameter we use to train and optimize our NN or machine learning model. Any bias and business requirements can be added to the error so that the model will be trained accordingly. \n",
    "\n",
    "\n",
    "<h1>Into to Regularization</h1>\n",
    "Regularization is a way to keep the model simple and prevent it from overfitting. \n",
    "\n",
    "Let's have a look at the below equation. \n",
    "\n",
    "h(x) = w3(x^3) + w2(x^2) + w1(x) + w0\n",
    "\n",
    "This equation shows that x with a high exponent will overfit and make the model more complex but gives a good value for the training and testing as the resultant curve passes through all the given points. \n",
    "\n",
    "Regularization will help you keep the model simple and make the weights of higher exponents zero or very low, keeping the model simple and preventing overfitting. \n",
    "\n",
    "Assume h(x) is out model function, and l_reg(w) = l(w) + l0(w)\n",
    "\n",
    "\n",
    "<strong><h2>L0:</h2></strong>  L0(w) results in the total no of non-zero weights. Using a non-zero number of parameters can be a helpful selection feature, with sparse features achieved when selection parameters are non-zero. \n",
    "\n",
    "L0: this can help in <b>parameter selection</b>, such as ignoring the parameters that have no or minimal impact on the output and only selecting the decision-making features.\n",
    "\n",
    "(Personal opinion) Here, the l0 can prefer a few weights with very high values over more weights with fewer values. \n",
    "\n",
    "\n",
    "<strong><h2>L1:</h2></strong> Is an absolute sum of all weights in the model. The size and value of the parameters are proportional to the model's complexity. So complex models have a big L1 norm, resulting in a huge loss function, indicating that this model is inadequate.\n",
    "\n",
    "L1 may not prevent overfitting, but the <b>important features will be given more weight</b>, and the normalization of parameters as per their scale will be achieved. <b>Feature selection</b> is done with it's help\n",
    "\n",
    "l1(w) = abs(w2 + w1 + w0)\n",
    "\n",
    "<b><h2>L2:</h2></b>  Is the sum of squares of all the weights in the model. L2 will reward the model when the weights are close to zero and below 1. As their squares will be almost zero. L2 will <b>prevent the model from overfitting. </b>\n",
    "\n",
    "l2(w) = (w2)^2 + (w1)^2 + (w0)^2\n",
    "\n",
    "<h1>Using l0, l1, l2 on GAN:</h1>\n",
    "\n",
    "Weight penalization approaches have been widely utilized in GAN models to avoid the mode collapse problem. There are two types of weight penalization strategies for GAN: weight penalty and weight normalization. Weight penalty methods use additional losses for the GAN target function, similar to the ridge regression and LASSO algorithms. As a result, the loss function with the weight penalty approach is the same as before, but instead of a single value, a norm of weight matrices is utilized. Weight normalization approaches, on the other hand, use a specialized training algorithm to update the weight matrices in GAN throughout training constantly.\n",
    "\n",
    "Evaluation of the L2 norm as a weight penalty approach; as a result, it was discovered that the L2 norm degrades performance. As a result of this discovery, unique approaches for training GAN are necessary, as the most common method for neural network models is ineffective. \n",
    "Various evaluations of L1 and L0 norms are being evaluated. As a result, these norms showed regularization effects during GAN training. Computational time is reduced compared to the non-regularized loss function.\n",
    "\n",
    "\n",
    "\n",
    "The l0, l1, and l2 impact the GAN, but it is usually not as desired or as effective as their performance in a regular CNN. So various combinations and different methods are explored for the regularization.\n",
    "\n",
    "In place of backward propogation, \"Backward Feature Correction\" is also recommended, will have to explore how and why it is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 2)</h1>\n",
    "\n",
    "<b>Contrast images with sharp geometric edge:</b>\n",
    "This property will create an easy solution. A high-resolution color image with a natural edge will have to consider many factors to create a loss function.\n",
    "\n",
    "My approach will be computing a basic PSNR to the generated and the sample image. As we need to minimize the loss, we use <b>-ve of PSNR.</b>\n",
    "\n",
    "<b>PSNR = 10Log10( (R^2)/MSE )</b>\n",
    "\n",
    "Here in place of R, we can use the range of the pixel map. The MSE is the mean square error for the pixel difference in the images. \n",
    "\n",
    "Justification: \n",
    "\n",
    "<b>Usage of MSE:</b> as we have a limited range of color pixels, the edges are shared, and any difference in the pixels can cause a significant change in the resulting image; we prevent such small or huge differences with the help of MSE. \n",
    "\n",
    "\n",
    "<b>Negative (PSNR):</b> PSNR is an excellent factor in measuring the compressed and reconstructed error quality. A negative value for it will be a great loss function to optimize the PSNR. \n",
    "\n",
    "\n",
    "<b>NOTE:</b> The upper and lower limits for the loss function are not considered. This may need to be tuned based on the Exact data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 3)</h1>\n",
    "\n",
    "A basic approach can be applying PSNR for each color scale. As the data set is animals in wildlife, this method can fail for many reasons. \n",
    "\n",
    "\n",
    "The approach will be using the loss function proposed or used in the paper \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.\"\n",
    "\n",
    "<b> Content loss: </b> We can utilize a loss function closer to perceptual similarity than pixel-wise losses. The updated Euclidean distance between the feature and the GAN loss is calculated. \n",
    "\n",
    "<img src=\"loss.png\">\n",
    "\n",
    "\n",
    "<b>JUSTIFICATION:</b> W, H being each node's respective feature maps of each respective node. This is a sum of loss with respective of weight and output of each node, This helps to keep the loss disturbuted over the whole image, and will also leat each node learn. Rather than having to calculate the whole image loss and passing through the NN to learn.\n",
    "\n",
    "\n",
    "<b> Adversarial loss: </b>\n",
    "\n",
    "We add the generative component of our GAN to the perceptual loss in addition to the content loss. Attempting to trick the discriminator network encourages our network to favor solutions based on various natural images. \n",
    "\n",
    "<b>L = Sigma( - LOG( D (G (I) ) ) )</b>\n",
    "\n",
    "<b>D (G (I) ):</b> The probability that the reconstructed image, G(I)\n",
    "being an actual image.\n",
    "\n",
    "As GAN cannot be only trained on content loss, because we are not expecting the GAN to generate images out of our natural images we use Adversarial loss. This loss helps the model to learn to create images that fall into the pool of natural images, enough to trick the discriminator network. The D() Gives the probability foe the generated image D(I) to be in the pool of natural image. So to optimize the Model the sum of negative logs has been added. \n",
    "\n",
    "This approach gives a bit of robustness, and all will give enough relaxation to generate HD images through GAN, where sharp edges can exist in fewer numbers and address a color image. \n",
    "\n",
    "\n",
    "<b>NOTE:</b> The loss function mentioned is from a paper for HD images, from the pool of alternatives, this suited the given case of \"Wild life in natural habitat\". A few basic alternatives are not as robust or detailed enough for our problem statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 4)</h1>\n",
    "\n",
    "<h2><b>A)</b></h2>Measures of Distributional Similarity: Is a method we can use closeness of q to p. \n",
    "\n",
    "A brute force can be an MSE, which will give the sum of the distance between the points. But as we can tell, a Distributional is not a point of measure but their scale and how the points are distributed to the other points.\n",
    "\n",
    "Before we see difference between lets see how to measure a disturbution.\n",
    "\n",
    "\n",
    "<b>Entropy:</b>\n",
    "\n",
    "The average level of \"information\" or \"uncertainty\" inherent in the variable's probable outcomes is the entropy of a random variable. Suppose you have a discrete random variable called x. p(x) being the value for x in the  \n",
    "Distribution.\n",
    "\n",
    "\n",
    "Entropy: - sigma( p(x)log(p(x)) )\n",
    "\n",
    "\n",
    "<b>Cross Entropy:</b>\n",
    "\n",
    "The amount of bits required to represent or transmit an average event from one distribution relative to another is calculated using the principle of entropy from information theory. \n",
    "\n",
    "The probability of the occurrences from P and Q can be used to determine cross-entropy.\n",
    "\n",
    "Cross Entropy: - sigma( p(x)log(q(x)) )\n",
    "\n",
    "The issue here is, this is not the measure between the two Distributional curves, but the no of bits needed, to represent one in place of other.\n",
    "\n",
    "\n",
    "\n",
    "<b>KL Divergence:</b>\n",
    "\n",
    "Measures a quantity that is quite comparable to cross-entropy. It calculates the average number of extra bits needed to express a message using Q rather than P, not the overall amount of bits. The negative sum of the probability of each event in P multiplied by the log of the probability of the event in Q over the probability of the event in P yields the K L divergence. \n",
    "\n",
    "\n",
    "KL(p, q) = Sigma( p(x) log (q(x) / p(x)) )\n",
    "\n",
    "The KL divergence is a common information-theoretic \"measure\" of the difference between two probability mass functions that have been used for distribution functions. The relative entropy, or the difference between cross-entropy and entropy, or any distance between the actual and anticipated probability distributions, is known as KL divergence.\n",
    "\n",
    "\n",
    "<h2><b>B)</b></h2>\n",
    "\n",
    "We rely on layers in GAN, L1, L2... Ln to produce a normal distribution.\n",
    "I am using the KL divergence as the loss function and adding any regularization factor to the loss function. \n",
    "\n",
    "We consider layer-wise training, in which the first layer of the learner generator L1 is trained to generate the lowest-resolution images X1, the second layer is trained to generate X2, the third layer is trained to generate X3, and so on. We divide the learning process into segments.\n",
    "\n",
    "\n",
    "We show that learning to match the moments between the generator's output distribution and the target distribution's actual distribution is sufficient for learning the output deconvolution layers. \n",
    "\n",
    "\n",
    "The output deconvolution layer can be intuitively expressed as a linear operator, y = wx. Each column of matrix A corresponds to the value of the distribution.\n",
    "\n",
    "Put another way, all that is required to learn the output deconvolution layer is for the GAN generator to match moments between its output distribution and the actual distribution. We show that a ReLU-type discriminator can distinguish the mismatch between the moments on images on the theoretical side. As a result, gradient descent ascent may be used to efficiently train the output deconvolution layers.\n",
    "\n",
    "\n",
    "Based on this approach, and our favorite back propagation, we train the whole GAN to produce an output that gets the P, Q distribution very close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 5)</h1>\n",
    "\n",
    "<b>InfoGAN:</b> Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.\n",
    "\n",
    "\n",
    "\n",
    "<b>Introduction:</b>\n",
    "\n",
    "Unsupervised learning can be defined as the general problem of extracting value from large amounts of unlabeled data. The purpose of representation learning, a prominent method for unsupervised learning, is to develop a representation that exposes significant semantic traits as easily decodable elements using unlabeled data. Generative modeling drives a substantial portion of unsupervised learning research. The variational autoencoder (VAE) and the generative adversarial network (GAN) are two of the most popular generative models. These findings show that generative modeling combined with a reciprocal information cost may be an effective method for learning disentangled representations. Then we'll look at GANs, which are the foundation of InfoGAN.\n",
    "\n",
    "<b>Related Work:</b>\n",
    "\n",
    "Unsupervised representation learning has a substantial corpus of research. The Skip-gram model, which inspired the skip-thought vectors and various techniques for unsupervised feature learning of images, has inspired many promising recent works. GANs were used to learn an image representation on code space that supports introductory linear algebra. Unlike the previously mentioned research that attempts to recover disentangled representations, InfoGAN does not require supervision. GAN learns a generator network G that generates samples from the generator distribution P. G by translating a noise variable z, P noise (z) into a sample G instead of explicitly assigning a probability to every x in the data distribution (z).\n",
    "\n",
    "<b>Mutual Information for Inducing Latent Codes:</b>\n",
    "\n",
    "The GAN algorithm employs a primary factored continuous input noise vector z, with no limits on how the generator might use it. Many domains break down naturally into a set of semantically significant causes of variation. Rather than employing a single unstructured noise vector, we propose splitting the input noise vector into two pieces in this paper. 1 z, which is viewed as a source of incompressible noise; 2 c, which we will refer to as the latent code, will target the data distribution's salient structured semantic properties. On the other hand, Standard GAN allows the generator to ignore the additional latent code c by finding a solution fulfilling P G (x|c) = P G (x).\n",
    "\n",
    "\n",
    "<b>Variational Mutual Information Maximization:</b>\n",
    "\n",
    "In reality, because it requires access to the posterior P (c|x), the mutual information term I(c; G(z, c) is difficult to maximize directly. Fortunately, by defining an auxiliary distribution Q(c|x) to approximate P (c|x), we can lower it's bound: Variational Information Maximization is a strategy for lower limiting mutual information. L I, in particular, can be improved. As a result, L I (G, Q) can be added to GAN's objectives without affecting the training procedure, and the resulting technique is dubbed Information Maximizing Generative Adversarial Networks (IMGN) (InfoGAN). When there are continuous variables in the latent code, a smaller is usually used to ensure that L I (G, Q), which now includes differential entropy, is on the same scale as GAN objectives. \n",
    "\n",
    "\n",
    "<b>Experiments:</b>\n",
    "\n",
    "\n",
    "Our experiments' primary purpose is to see if mutual information can be efficiently maximized. The second goal is to see if InfoGAN can acquire disentangled and interpretable representations by using the generator to change only one latent factor at a time to see if changing that factor only causes one type of semantic variance in generated images. DC-IGN also employs this method to assess their learned representations on 3D image datasets, then compared using InfoGAN. We train InfoGAN on the MNIST dataset with a uniform categorical distribution on latent codes c Cat(K = 10, p = 0.1) to see if the mutual information between latent codes c and generated images G(z, c) can be maximized efficiently with the proposed method. The lower bound L I (G, Q) is immediately maximized to H(c) 2.30, indicating that the constraint is tight and maximum mutual information has been reached.\n",
    "\n",
    "\n",
    "<b>Mutual Information Maximization:</b>\n",
    "\n",
    "When the generator is not expressly urged to maximize the mutual information with the latent codes, we train a regular GAN with an auxiliary distribution Q as a baseline. We assume that Q reasonably approximates the genuine posterior P (c|x) because we utilize expressive neural networks to parametrize Q. So there is little mutual information between latent codes and generated images in standard GAN. Even while we have not found such a case in our studies, we highlight that there may be more reciprocal information between latent codes and output images with different neural network architectures. This comparison shows that there is no certainty that the generator will employ the latent codes in a typical GAN.\n",
    "\n",
    "\n",
    "<b>Disentangled Representation:</b>\n",
    "\n",
    "On MNIST, we chose to model the latent codes with one categorical code, c 1 Cat(K = 10, p = 0.1), which can model discontinuous variation in data, and two serial codes, c 2 and c 3 Unif(1, 1), which may capture continuous changes. Serial codes c 2 and c 3 record continuous style variations: c 2 models digit rotation while c 3 controls width. In this experiment, we used four categorical codes (c 1, c 2, c 3, c 4 Cat(K = 20, p = 0.05) and one continuous code (c 5 Unif(1, 1) to model the latent components. We then tested InfoGAN on the Street View House Number (SVHN) dataset. It is substantially more difficult to train an interpretable representation since it is noisy, contains photos of variable-resolution and distracting digits, and lacks many variations of the same item. We use four ten-dimensional categorical variables and two uniform continuous variables as latent codes in this experiment. To provide a direct comparison, we present the representation that most closely reflects prior supervised results out of 5 random runs for each factor.\n",
    "\n",
    "\n",
    "<b>Conclusion:</b>\n",
    "\n",
    "Information Maximizing Generative Adversarial Networks is a representation learning algorithm introduced in this paper (InfoGAN). Generator: Instead of drawing from the actual data distribution, the \"sleep\" phase updates the auxiliary distribution Q by \"dreaming\" up samples from the current generator distribution. As a result, we can observe that the update step in the Wake-Sleep algorithm is the \"sleep\" phase update while optimizing the surrogate loss L I w.r.t. Q. When we optimize L I w.r.t., InfoGAN differs from Wake-Sleep. Our method can be understood as a \"Sleep-Sleep\" algorithm because InfoGAN also updates the generator during the \"sleep\" phase. This interpretation underscores the difference between InfoGAN and prior generative modeling techniques. The generator is encouraged to convey information in latent codes, implying that the same approach can be applied to other generative models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
